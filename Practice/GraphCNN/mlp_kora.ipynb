{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  !pip install -q tensorflow-gpu>=2.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet neural-structured-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Version:  2.0.0\nEager mode:  True\nGPU is NOT AVAILABLE\n"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import neural_structured_learning as nsl\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Resets notebook state\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "print(\"Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\"GPU is\", \"available\" if tf.test.is_gpu_available() else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "cora/\ncora/README\ncora/cora.content\ncora/cora.cites\n"
    }
   ],
   "source": [
    "!wget --quiet -P /tmp https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\n",
    "!tar -C /tmp -xvzf /tmp/cora.tgz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "--2019-10-24 05:06:03--  https://raw.githubusercontent.com/tensorflow/neural-structured-learning/master/neural_structured_learning/examples/preprocess/cora/preprocess_cora_dataset.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)...151.101.0.133, 151.101.64.133, 151.101.128.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 11419 (11K) [text/plain]\nSaving to: ‘preprocess_cora_dataset.py.1’\n\npreprocess_cora_dat 100%[===================>]  11.15K  --.-KB/s    in 0s      \n\n2019-10-24 05:06:03 (67.9 MB/s) - ‘preprocess_cora_dataset.py.1’ saved [11419/11419]\n\npreprocess_cora_dataset.py:130: DeprecationWarning: 'U' mode is deprecated\n  with open(in_file, 'rU') as cora_content:\nReading graph file: /tmp/cora/cora.cites...\nDone reading 5429 edges from: /tmp/cora/cora.cites (0.02 seconds).\nMaking all edges bi-directional...\nDone (0.01 seconds). Total graph nodes: 2708\nJoining seed and neighbor tf.train.Examples with graph edges...\nDone creating and writing 2155 merged tf.train.Examples (1.90 seconds).\nOut-degree histogram: [(1, 386), (2, 468), (3, 452), (4, 309), (5, 540)]\nOutput training data written to TFRecord file: /tmp/cora/train_merged_examples.tfr.\nOutput test data written to TFRecord file: /tmp/cora/test_examples.tfr.\nTotal running time: 0.07 minutes.\n"
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/tensorflow/neural-structured-learning/master/neural_structured_learning/examples/preprocess/cora/preprocess_cora_dataset.py\n",
    "\n",
    "!python preprocess_cora_dataset.py \\\n",
    "--input_cora_content=/tmp/cora/cora.content \\\n",
    "--input_cora_graph=/tmp/cora/cora.cites \\\n",
    "--max_nbrs=5 \\\n",
    "--output_train_data=/tmp/cora/train_merged_examples.tfr \\\n",
    "--output_test_data=/tmp/cora/test_examples.tfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment dataset\n",
    "TRAIN_DATA_PATH = '/tmp/cora/train_merged_examples.tfr'\n",
    "TEST_DATA_PATH = '/tmp/cora/test_examples.tfr'\n",
    "\n",
    "### Constants used to identify neighbor features in the input.\n",
    "NBR_FEATURE_PREFIX = 'NL_nbr_'\n",
    "NBR_WEIGHT_SUFFIX = '_weight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams(object):\n",
    "  \"\"\"Hyperparameters used for training.\"\"\"\n",
    "  def __init__(self):\n",
    "    ### dataset parameters\n",
    "    self.num_classes = 7\n",
    "    self.max_seq_length = 1433\n",
    "    ### neural graph learning parameters\n",
    "    self.distance_type = nsl.configs.DistanceType.L2\n",
    "    self.graph_regularization_multiplier = 0.1\n",
    "    self.num_neighbors = 1\n",
    "    ### model architecture\n",
    "    self.num_fc_units = [50, 50]\n",
    "    ### training parameters\n",
    "    self.train_epochs = 100\n",
    "    self.batch_size = 128\n",
    "    self.dropout_rate = 0.5\n",
    "    ### eval parameters\n",
    "    self.eval_steps = None  # All instances in the test set are evaluated.\n",
    "\n",
    "HPARAMS = HParams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_example(example_proto):\n",
    "  \"\"\"Extracts relevant fields from the `example_proto`.\n",
    "\n",
    "  Args:\n",
    "    example_proto: An instance of `tf.train.Example`.\n",
    "\n",
    "  Returns:\n",
    "    A pair whose first value is a dictionary containing relevant features\n",
    "    and whose second value contains the ground truth labels.\n",
    "  \"\"\"\n",
    "  # The 'words' feature is a multi-hot, bag-of-words representation of the\n",
    "  # original raw text. A default value is required for examples that don't\n",
    "  # have the feature.\n",
    "  feature_spec = {\n",
    "      'words':\n",
    "          tf.io.FixedLenFeature([HPARAMS.max_seq_length],\n",
    "                                tf.int64,\n",
    "                                default_value=tf.constant(\n",
    "                                    0,\n",
    "                                    dtype=tf.int64,\n",
    "                                    shape=[HPARAMS.max_seq_length])),\n",
    "      'label':\n",
    "          tf.io.FixedLenFeature((), tf.int64, default_value=-1),\n",
    "  }\n",
    "  # We also extract corresponding neighbor features in a similar manner to\n",
    "  # the features above.\n",
    "  for i in range(HPARAMS.num_neighbors):\n",
    "    nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, i, 'words')\n",
    "    nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, i, NBR_WEIGHT_SUFFIX)\n",
    "    feature_spec[nbr_feature_key] = tf.io.FixedLenFeature(\n",
    "        [HPARAMS.max_seq_length],\n",
    "        tf.int64,\n",
    "        default_value=tf.constant(\n",
    "            0, dtype=tf.int64, shape=[HPARAMS.max_seq_length]))\n",
    "\n",
    "    # We assign a default value of 0.0 for the neighbor weight so that\n",
    "    # graph regularization is done on samples based on their exact number\n",
    "    # of neighbors. In other words, non-existent neighbors are discounted.\n",
    "    feature_spec[nbr_weight_key] = tf.io.FixedLenFeature(\n",
    "        [1], tf.float32, default_value=tf.constant([0.0]))\n",
    "\n",
    "  features = tf.io.parse_single_example(example_proto, feature_spec)\n",
    "\n",
    "  labels = features.pop('label')\n",
    "  return features, labels\n",
    "\n",
    "\n",
    "def make_dataset(file_path, training=False):\n",
    "  \"\"\"Creates a `tf.data.TFRecordDataset`.\n",
    "\n",
    "  Args:\n",
    "    file_path: Name of the file in the `.tfrecord` format containing\n",
    "      `tf.train.Example` objects.\n",
    "    training: Boolean indicating if we are in training mode.\n",
    "\n",
    "  Returns:\n",
    "    An instance of `tf.data.TFRecordDataset` containing the `tf.train.Example`\n",
    "    objects.\n",
    "  \"\"\"\n",
    "  dataset = tf.data.TFRecordDataset([file_path])\n",
    "  if training:\n",
    "    dataset = dataset.shuffle(10000)\n",
    "  dataset = dataset.map(parse_example)\n",
    "  dataset = dataset.batch(HPARAMS.batch_size)\n",
    "  return dataset\n",
    "\n",
    "\n",
    "train_dataset = make_dataset(TRAIN_DATA_PATH, training=True)\n",
    "test_dataset = make_dataset(TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Feature list: ['NL_nbr_0_weight', 'NL_nbr_0_words', 'words']\nBatch of inputs: tf.Tensor(\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [1 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]], shape=(128, 1433), dtype=int64)\nBatch of neighbor inputs: tf.Tensor(\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]], shape=(128, 1433), dtype=int64)\nBatch of neighbor weights: tf.Tensor(\n[1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n 1. 1. 1. 1. 1. 1. 1. 1.], shape=(128,), dtype=float32)\nBatch of labels: tf.Tensor(\n[3 5 0 4 3 2 1 2 6 2 2 2 5 4 2 1 5 6 3 2 1 0 5 4 3 5 5 0 3 1 1 2 6 5 2 2 2\n 2 2 1 6 1 6 2 0 1 1 3 6 1 5 4 5 3 4 1 1 1 3 6 2 3 2 4 3 2 0 1 2 2 1 0 2 6\n 1 1 0 3 1 2 2 6 6 3 6 2 6 2 4 2 6 2 0 6 1 1 4 2 5 4 2 1 4 2 3 5 1 0 4 2 0\n 3 2 0 4 2 2 2 2 4 0 1 3 2 1 2 3 1], shape=(128,), dtype=int64)\n"
    }
   ],
   "source": [
    "for feature_batch, label_batch in train_dataset.take(1):\n",
    "  print('Feature list:', list(feature_batch.keys()))\n",
    "  print('Batch of inputs:', feature_batch['words'])\n",
    "  nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, 0, 'words')\n",
    "  nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, 0, NBR_WEIGHT_SUFFIX)\n",
    "  print('Batch of neighbor inputs:', feature_batch[nbr_feature_key])\n",
    "  print('Batch of neighbor weights:',\n",
    "        tf.reshape(feature_batch[nbr_weight_key], [-1]))\n",
    "  print('Batch of labels:', label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Feature list: ['NL_nbr_0_weight', 'NL_nbr_0_words', 'words']\nBatch of inputs: tf.Tensor(\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]], shape=(128, 1433), dtype=int64)\nBatch of neighbor inputs: tf.Tensor(\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]], shape=(128, 1433), dtype=int64)\nBatch of neighbor weights: tf.Tensor(\n[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0.], shape=(128,), dtype=float32)\nBatch of labels: tf.Tensor(\n[5 2 2 2 1 2 6 3 2 3 6 1 3 6 4 4 2 3 3 0 2 0 5 2 1 0 6 3 6 4 2 2 3 0 4 2 2\n 2 2 3 2 2 2 0 2 2 2 2 4 2 3 4 0 2 6 2 1 4 2 0 0 1 4 2 6 0 5 2 2 3 2 5 2 5\n 2 3 2 2 2 2 2 6 6 3 2 4 2 6 3 2 2 6 2 4 2 2 1 3 4 6 0 0 2 4 2 1 3 6 6 2 6\n 6 6 1 4 6 4 3 6 6 0 0 2 6 2 4 0 0], shape=(128,), dtype=int64)\n"
    }
   ],
   "source": [
    "for feature_batch, label_batch in test_dataset.take(1):\n",
    "  print('Feature list:', list(feature_batch.keys()))\n",
    "  print('Batch of inputs:', feature_batch['words'])\n",
    "  nbr_feature_key = '{}{}_{}'.format(NBR_FEATURE_PREFIX, 0, 'words')\n",
    "  nbr_weight_key = '{}{}{}'.format(NBR_FEATURE_PREFIX, 0, NBR_WEIGHT_SUFFIX)\n",
    "  print('Batch of neighbor inputs:', feature_batch[nbr_feature_key])\n",
    "  print('Batch of neighbor weights:',\n",
    "        tf.reshape(feature_batch[nbr_weight_key], [-1]))\n",
    "  print('Batch of labels:', label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}